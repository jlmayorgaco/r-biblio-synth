library(extrafont)

font_import()  


# ---------------------------------------------------------------------------- #
# Function: Generate Report for Document Types Analysis
# ---------------------------------------------------------------------------- #
fn_m1_mtrc1_generate_document_types_report <- function(v_document_types) {
  # Ensure the output directory exists
  report_dir <- "results/M1_Main_Information/reports"
  if (!dir.exists(report_dir)) {
    dir.create(report_dir, recursive = TRUE)
  }

  # Create a data frame for document types
  df <- data.frame(
    Document_Type = names(v_document_types),
    Count = sapply(v_document_types, function(x) ifelse(length(x) == 0, 0, x))
  )

  # Filter out document types with zero count and apply label mapping
  df <- df[df$Count > 0, ]
  df$Document_Type <- LABEL_MAPPING[df$Document_Type]

  # Sort the data by count in descending order
  df <- df[order(-df$Count), ]

  # Calculate percentages
  total_count <- sum(df$Count)
  df$Percentage <- round((df$Count / total_count) * 100, 2)

  # Generate a plain text report
  txt_report <- paste0(
    "Document Types Distribution Report\n",
    "==================================\n\n",
    "Total Articles: ", total_count, "\n\n",
    "Breakdown by Document Type:\n\n",
    paste(
      sprintf(
        "%-20s: %d (%.2f%%)", 
        df$Document_Type, df$Count, df$Percentage
      ),
      collapse = "\n"
    ),
    "\n\nGenerated by fn_m1_mtrc1_generate_document_types_report"
  )

  # Save the plain text report
  txt_file_path <- file.path(report_dir, "M1_Document_Types_Report.txt")
  writeLines(txt_report, txt_file_path)

  # Generate a LaTeX report
  tex_report <- paste0(
    "\\documentclass[12pt]{article}\n",
    "\\usepackage{geometry}\n",
    "\\usepackage{graphicx}\n",
    "\\geometry{a4paper, margin=1in}\n",
    "\\begin{document}\n",
    "\\title{Document Types Distribution Report}\n",
    "\\author{}\n",
    "\\date{\\today}\n",
    "\\maketitle\n",
    "\\section*{Overview}\n",
    "This report summarizes the distribution of document types in the dataset.\n",
    "The total number of articles analyzed is \\textbf{", total_count, "}.\n\n",
    "\\section*{Breakdown by Document Type}\n",
    "\\begin{itemize}\n",
    paste(
      sprintf(
        "\\item \\textbf{%s}: %d (%.2f\\%%)", 
        df$Document_Type, df$Count, df$Percentage
      ),
      collapse = "\n"
    ),
    "\n\\end{itemize}\n\n",
    "\\section*{Visualization}\n",
    "The distribution of document types is visualized in the accompanying pie chart.\n",
    "\\includegraphics[width=0.8\\textwidth]{../figures/M1_G2_DOCUMENT_TYPES_PIE_PLOT_PNG.png}\n",
    "\\end{document}"
  )

  # Save the LaTeX report
  tex_file_path <- file.path(report_dir, "M1_Document_Types_Report.tex")
  writeLines(tex_report, tex_file_path)

  # Log success
  message("[INFO] Document types reports generated successfully:")
  message("  - Plain text report: ", txt_file_path)
  message("  - LaTeX report: ", tex_file_path)
}
library(glue)

generate_document_types_report <- function(df, output_path) {
  # Ensure percentages are calculated
  total_count <- sum(df$Count)
  df$Percentage <- (df$Count / total_count) * 100
  
  # Determine key insights
  dominant_type <- df$Document_Type[which.max(df$Percentage)]
  dominant_percentage <- max(df$Percentage)
  evenly_distributed <- all(df$Percentage < 40) # No type has more than 40%

  # Generate conclusions dynamically
  if (dominant_percentage > 50) {
    conclusions <- glue(
      "The analysis reveals that the document type \\textbf{{{dominant_type}}} dominates the dataset, ",
      "representing \\textbf{{{round(dominant_percentage, 2)}}}\\% of all articles. ",
      "This suggests that most articles in this collection are focused on this specific type."
    )
  } else if (evenly_distributed) {
    conclusions <- glue(
      "The dataset shows an even distribution of document types, with no single type exceeding 40\\% of the total. ",
      "This indicates a diverse set of article formats in the collection."
    )
  } else {
    conclusions <- glue(
      "The document types are varied, with \\textbf{{{dominant_type}}} being the most common, ",
      "accounting for \\textbf{{{round(dominant_percentage, 2)}}}\\% of the total. ",
      "Other document types contribute significantly as well, indicating a balanced dataset."
    )
  }

  # Create LaTeX content
  latex_template <- "
\\documentclass[12pt]{article}
\\usepackage{geometry}
\\usepackage{graphicx}
\\geometry{a4paper, margin=1in}
\\begin{document}
\\title{Document Types Distribution Report}
\\author{}
\\date{\\today}
\\maketitle
\\section*{Overview}
The total number of articles analyzed is \\textbf{{{total_count}}}.
\\section*{Breakdown by Document Type}
\\begin{itemize}
{item_list}
\\end{itemize}
\\section*{Conclusions}
{conclusions}
\\section*{Visualization}
The distribution of document types is visualized in the accompanying pie chart:
\\includegraphics[width=0.8\\textwidth]{../figures/M1_G2_DOCUMENT_TYPES_PIE_PLOT_PNG.png}
\\end{document}
  "

  # Generate item list for LaTeX
  item_list <- paste(
    glue("\\item \\textbf{{{df$Document_Type}}}: {df$Count} ({round(df$Percentage, 2)}\\%)"),
    collapse = "\n"
  )

  # Fill the template with content
  content <- glue(latex_template,
                  total_count = total_count,
                  item_list = item_list,
                  conclusions = conclusions
  )

  # Save the LaTeX file
  writeLines(content, output_path)
}









plot_dual_axis_bar_chart <- function(most_prod_countries, tc_per_country) {
  tryCatch({
    # Step 1: Combine Data
    message("[DEBUG] Combining data...")
    combined_data <- merge(most_prod_countries, tc_per_country, by = "Country", all = TRUE) %>%
      mutate(
        Articles = suppressWarnings(as.numeric(Articles)),
        `Total Citations` = suppressWarnings(as.numeric(`Total Citations`))
      )

    # Step 2: Check for Missing or Invalid Data
    if (any(is.na(combined_data$Articles)) || any(is.na(combined_data$`Total Citations`))) {
      message("[INFO] Removing rows with NA in 'Articles' or 'Total Citations'.")
      combined_data <- combined_data[complete.cases(combined_data[, c("Articles", "Total Citations")]), ]
    }

    # Ensure there is valid data
    if (nrow(combined_data) == 0) {
      stop("[ERROR] No valid data available for plotting after cleaning.")
    }

    # Step 3: Reshape Data
    message("[DEBUG] Reshaping data for plotting...")
    combined_long <- combined_data %>%
      pivot_longer(cols = c(Articles, `Total Citations`), names_to = "Metric", values_to = "Value") %>%
      mutate(Value = suppressWarnings(as.numeric(Value)))


    # Check for Invalid Values
    if (any(is.na(combined_long$Value))) {
      message("[INFO] Removing rows with NA in 'Value' column.")
      combined_long <- combined_long[!is.na(combined_long$Value), ]
    }

    # Ensure there is valid data to plot
    if (nrow(combined_long) == 0) {
      stop("[ERROR] No valid data available for plotting after reshaping.")
    }

    # Step 4: Define Colors for the Metrics
    metric_colors <- c("Articles" = THEME_COLORS$Main[1], "Total Citations" = THEME_COLORS$Main[2])

    # Step 5: Create Dual-axis Bar Chart
    message("[DEBUG] Creating dual-axis bar chart...")
    dual_bar_chart <- ggplot(combined_long, aes(x = reorder(Country, -Value), y = Value, fill = Metric)) +
      geom_bar(stat = "identity", position = "dodge", width = 0.7) +
      scale_fill_manual(values = metric_colors) +
      labs(
        title = "Comparison of Productivity and Total Citations",
        x = "Country",
        y = "Value"
      ) +
      theme_minimal() +
      theme(
        plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title = element_text(size = 12),
        legend.title = element_blank(),
        legend.position = "top"
      )

    # Debug: Validate Plot Object
    if (!inherits(dual_bar_chart, "ggplot")) {
      stop("[ERROR] Generated object is not a valid ggplot object.")
    } else {
      message("[DEBUG] Dual-axis bar chart object created successfully.")
    }

    # Step 6: Save Plot
    message("[DEBUG] Saving dual-axis bar chart...")
    save_plot(dual_bar_chart, "PT_3_Comparison_Productivity_vs_Impact_Bar_Chart", width = 10, height = 6, dpi = 300)

    # Step 7: Save Insights
    insight <- "This chart highlights which countries have a balance or imbalance between their productivity and total citations."
    write(insight, file = "results/Comparison_Productivity_vs_Impact_Insights.txt")

    message("[INFO] Dual-axis bar chart created successfully.")
    return(dual_bar_chart)
  }, error = function(e) {
    message("[ERROR] Failed to create dual-axis bar chart: ", e$message)
    return(NULL)
  })
}



library(ggplot2)
library(dplyr)
library(splines)
plot_lorenz_curve_overlay <- function(most_prod_countries, tc_per_country) {
  tryCatch({
    # Combine data
    combined_data <- merge(most_prod_countries, tc_per_country, by = "Country") %>%
      mutate(
        Articles = as.numeric(Articles),               # Ensure Articles is numeric
        `Total Citations` = as.numeric(`Total Citations`) # Ensure Total Citations is numeric
      ) %>%
      arrange(desc(Articles)) %>%
      mutate(
        CumArticles = cumsum(Articles) / sum(Articles),
        CumCitations = cumsum(`Total Citations`) / sum(`Total Citations`),
        CountryRank = row_number() / n()
      )

    # Dynamically extract column names
    required_columns <- colnames(combined_data)

    # Create endpoint rows with matching columns
    endpoint_rows <- data.frame(
      CumArticles = c(0, 1),
      CumCitations = c(0, 1),
      CountryRank = c(0, 1)
    )

    # Add any missing columns to endpoint_rows
    for (col in setdiff(required_columns, colnames(endpoint_rows))) {
      endpoint_rows[[col]] <- NA  # Add missing columns with NA
    }

    # Ensure column order matches
    endpoint_rows <- endpoint_rows[, required_columns]

    # Append endpoint rows to combined_data
    combined_data <- rbind(endpoint_rows, combined_data)

    # Ensure data is sorted for monotonic Lorenz curve
    combined_data <- combined_data %>%
      arrange(CumArticles, CumCitations, CountryRank)

    # Create Lorenz curve
    lorenz_curve <- ggplot(combined_data) +
      geom_line(aes(x = CumArticles, y = CountryRank, color = "Articles"), linewidth = 1) +
      geom_line(aes(x = CumCitations, y = CountryRank, color = "Citations"), linewidth = 1) +
      geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black", linewidth = 0.8) + # 1:1 line
      scale_color_manual(
        values = c("Articles" = "#1b9e77", "Citations" = "#d95f02"),
        labels = c("Articles", "Citations")
      ) +
      labs(
        title = "Lorenz Curve: Articles vs. Citations Distribution",
        subtitle = "Comparison of cumulative distributions by country",
        x = "Cumulative Proportion of Metric",
        y = "Cumulative Proportion of Countries"
      ) +
      ieee_theme +
      theme(
        plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        plot.subtitle = element_text(size = 12, hjust = 0.5, face = "italic"),
        axis.title = element_text(size = 12, face = "bold"),
        axis.text = element_text(size = 10),
        legend.position = "right",
        legend.title = element_blank(),
        legend.text = element_text(size = 10)
      )

    # Save the plot
    save_plot(lorenz_curve, "PT_3_Lorenz_Curve_Comparison", width = 10, height = 6, dpi = 600)

    # Save insights
    insight <- paste(
      "The Lorenz curve illustrates the disparity in the distribution of articles and citations among countries.",
      "The dashed 1:1 line indicates perfect equality. Deviations from this line highlight the degree of inequality."
    )
    write(insight, file = "results/Lorenz_Curve_Insights.txt")

    message("[INFO] Lorenz curve plot created successfully.")
    return(lorenz_curve)

  }, error = function(e) {
    message("[ERROR] Failed to create Lorenz curve overlay: ", e$message)
  })
}





# Function 4: Treemap with Combined Metrics
plot_combined_treemap <- function(most_prod_countries, tc_per_country) {
  # Combine data
  combined_data <- merge(most_prod_countries, tc_per_country, by = "Country")

  # Create treemap
  combined_treemap <- ggplot(combined_data, aes(
    area = Articles,
    fill = `Total Citations`,
    label = paste(Country, "\nArticles:", Articles, "\nCitations:", `Total Citations`)
  )) +
    geom_treemap() +
    geom_treemap_text(fontface = "bold", colour = "white", place = "centre", grow = TRUE) +
    scale_fill_viridis_c(option = "plasma", name = "Total Citations") +
    labs(
      title = "Treemap: Articles vs. Citations",
      fill = "Total Citations"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5)
    )

  # Save plot
  save_plot("PT_3_Treemap_Articles_vs_Citations", combined_treemap, width = 10, height = 6, dpi = 300)

  # Save insights
  insight <- "The treemap visually combines productivity and impact, with area representing articles and color intensity representing citations."
  write(insight, file = "results/Treemap_Insights.txt")

  return(combined_treemap)
}





















wc_3x3_preprocess_data <- function(extracted_data) {
  library(dplyr)
  
  # Extract years and keywords
  years <- extracted_data$all_keywords$Years
  keywords <- extracted_data$all_keywords$Keywords
  
  # Create a data frame for easier manipulation
  data <- data.frame(year = years, keywords = keywords, stringsAsFactors = FALSE)
  
  # Define the overall range of years
  min_year <- floor(min(data$year, na.rm = TRUE)) # Ensure the minimum year is an integer
  max_year <- ceiling(max(data$year, na.rm = TRUE)) # Ensure the maximum year is an integer
  
  # Create 8 evenly distributed year ranges
  year_breaks <- seq(min_year, max_year, length.out = 9) # 8 ranges -> 9 breaks
  year_breaks <- unique(floor(year_breaks)) # Remove duplicates after rounding

  # Check if the breaks are still unique
  if (length(year_breaks) < 9) {
    # Adjust the sequence to create unique breaks manually
    year_breaks <- seq(min_year, max_year, length.out = 9)
    year_breaks <- unique(as.integer(round(year_breaks))) # Convert to integers and remove duplicates
  }


  year_labels <- paste(head(year_breaks, -1), tail(year_breaks, -1) - 1, sep = "-") # Labels like "1965-1971"
  
  # Ensure the last range includes the final year
  year_labels[length(year_labels)] <- paste(
    year_breaks[length(year_breaks) - 1], year_breaks[length(year_breaks)], sep = "-"
  )
  
  # Assign each year to the appropriate chunk
  data$chunk <- cut(data$year, breaks = year_breaks, labels = year_labels, include.lowest = TRUE)
  
  # Combine all keywords across all chunks
  all_words <- unlist(strsplit(data$keywords, " "))
  all_words <- all_words[all_words != ""] # Remove empty strings
  
  # Count word frequencies
  word_freq <- table(all_words)
  word_freq <- as.data.frame(word_freq, stringsAsFactors = FALSE)
  colnames(word_freq) <- c("word", "freq")
  
  # Get the top 100 most frequent words
  top_100_words <- word_freq[order(-word_freq$freq), ]$word[1:10]
  
  # Debugging: Check the chunks and word frequency
  message("[DEBUG] Year Ranges for Chunks: ")
  print(year_labels)
  message("[DEBUG] Assigned Chunks: ")
  print(table(data$chunk))
  message("[DEBUG] Word Frequency: ")
  print(head(word_freq))
  message("[DEBUG] Top 100 Words: ")
  print(top_100_words)
  
  # Return data, word_freq, and top_100_words
  return(list(
    data = data,
    word_freq = word_freq,
    top_100_words = top_100_words
  ))
}







wc_3x3_create_wordcloud_data <- function(data, word_freq, top_100_words) {
  library(wordcloud)
  library(grid)

  # Initialize lists to store grobs and chunk details
  grobs <- list()
  chunks_details <- list()

  # Debugging: Check `word_freq` and `top_100_words`
  message("[DEBUG] Structure of word_freq:")
  print(str(word_freq))
  message("[DEBUG] top_100_words content:")
  print(top_100_words)

  # Ensure `word_freq` and `top_100_words` are valid
  if (is.null(word_freq) || nrow(word_freq) == 0) {
    stop("[ERROR] word_freq is empty or NULL.")
  }

  if (length(top_100_words) == 0) {
    stop("[ERROR] top_100_words is empty.")
  }

  # Process common words once
  common_word_counts <- word_freq[word_freq$word %in% top_100_words, , drop = FALSE]
  if (nrow(common_word_counts) == 0) {
    stop("[ERROR] No common words found in word_freq.")
  }
  common_word_counts$freq <- as.numeric(common_word_counts$freq)
  common_word_counts <- common_word_counts[order(-common_word_counts$freq), ]

  # Create a data frame for common keywords
  common_keywords_df <- data.frame(
    word = common_word_counts$word,
    count = common_word_counts$freq,
    stringsAsFactors = FALSE
  )

  # Debugging
  message("[DEBUG] Sorted common keywords:")
  print(head(common_keywords_df))

  # Generate word cloud for each chunk
  chunks_total <- length(levels(data$chunk))
  for (chunk_index in seq_along(levels(data$chunk))) {
    chunk <- levels(data$chunk)[chunk_index]

    # Filter data for the current chunk
    chunk_data <- data[data$chunk == chunk, ]

    # Skip if no keywords are available
    if (nrow(chunk_data) == 0 || all(is.na(chunk_data$keywords)) || all(chunk_data$keywords == "")) {
      message(sprintf("[INFO] Skipping chunk %d of %d: %s", chunk_index, chunks_total, chunk))
      next
    }

    # Count keyword frequencies
    keyword_counts <- table(unlist(strsplit(chunk_data$keywords, " ")))
    keyword_counts <- as.data.frame(keyword_counts, stringsAsFactors = FALSE)
    colnames(keyword_counts) <- c("word", "freq")
    keyword_counts$freq <- as.numeric(keyword_counts$freq)
    keyword_counts <- keyword_counts[order(-keyword_counts$freq), ]

    # Generate word cloud grob in-memory
    gradient_color <- colorRampPalette(c("white", c("blue", "green", "purple", "orange", "red")[chunk_index]))
    grobs[[as.character(chunk)]] <- textGrob(paste(keyword_counts$word, collapse = " "),
                                             gp = gpar(col = gradient_color(1), fontsize = 12))

    # Save chunk details
    chunks_details[[paste0("chunk_", chunk)]] <- keyword_counts

    # Progress message
    progress_bar <- paste0("[", paste(rep("=", chunk_index), collapse = ""),
                           paste(rep(" ", chunks_total - chunk_index), collapse = ""), "]")
    message(sprintf("[INFO] Processing chunk %d of %d: %s", chunk_index, chunks_total, progress_bar))
  }

  # Generate the common words word cloud for the center
  grobs[["center"]] <- textGrob(
    paste(common_keywords_df$word, collapse = " "),
    gp = gpar(col = "black", fontsize = 16)
  )
  chunks_details[["common_words"]] <- common_keywords_df

  return(list(grobs = grobs, chunks_details = chunks_details))
}




wc_3x3_plot_and_save_results <- function(grobs, chunks_details) {
  library(gridExtra)
  library(jsonlite)

  # Generate correct labels
  labels <- c(
    sapply(chunks_details[1:8], function(chunk_df) chunk_df$year_range[1]),
    "Common Words"
  )

  message("[DEBUG] Labels for Word Cloud Grid: ")
  print(labels)

  grid_list <- lapply(seq_along(grobs), function(i) {
    grob <- grobs[[i]]
    label <- labels[i]
    gTree(children = gList(
      grob,
      textGrob(label, x = 0.5, y = 0.05, just = "center", gp = gpar(fontsize = 12))
    ))
  })

  output_file <- "results/M1_Main_Information/figures/M1_All_Keywords_Wordcloud_Grid.png"
  png(output_file, width = 1800, height = 1800)
  grid.arrange(grobs = grid_list, nrow = 3, ncol = 3)
  dev.off()
  message("[INFO] 3x3 Wordcloud grid saved successfully at: ", output_file)

  json_file <- "results/M1_Main_Information/figures/M1_All_Keywords_Chunks.json"
  write_json(chunks_details, json_file, pretty = TRUE)
  message("[INFO] Chunk details saved successfully at: ", json_file)
}

